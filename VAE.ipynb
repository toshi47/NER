{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOkDLxeDtDWrzkrtgA5K2C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toshi47/NER/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFw2eE-uD8cu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from keras.layers import Input, Dense, Lambda\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Single fully-connected neural layer as encoder and decoder\n",
        "df = pd.read_csv('/content/drive//My Drive/NER/my_small.csv')\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "original_dim= df.shape[1]-2\n",
        "print(original_dim)\n",
        "input_shape = (original_dim, )\n",
        "print(input_shape)\n",
        "intermediate_dim = int(original_dim/2)\n",
        "batch_size = 50\n",
        "latent_dim = 25\n",
        "epochs = 150\n",
        "epsilon_std = 1.0\n",
        "\n",
        "x = Input(shape=(original_dim,))\n",
        "h = Dense(intermediate_dim, activation='relu')(x)\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std)\n",
        "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "# we instantiate these layers separately so as to reuse them later\n",
        "decoder_h = Dense(intermediate_dim, activation='relu')\n",
        "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
        "h_decoded = decoder_h(z)\n",
        "x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "# instantiate VAE model\n",
        "vae = Model(x, x_decoded_mean)\n",
        "\n",
        "# Compute VAE loss\n",
        "xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "vae_loss = K.mean(xent_loss + kl_loss)\n",
        "\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "vae.summary()\n",
        "\n",
        "# train the VAE on MNIST digits\n",
        "\n",
        "print(\"Size of the dataframe: \", df.shape);\n",
        "#print(df.columns)\n",
        "#df.info()\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "df_values = df.drop(columns=['word', 'tag'], axis=1)\n",
        "df_norm = scaler.fit_transform(df_values)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_norm, df_norm,\n",
        "                                                    test_size=0.33, random_state=42)\n",
        "\n",
        "vae.fit(X_train,\n",
        "        shuffle=True,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=2,\n",
        "        validation_data=(X_test, None))\n",
        "\n",
        "# build a model to project inputs on the latent space\n",
        "encoder = Model(x, z_mean)\n",
        "print(X_test.shape)\n",
        "print(X_test[0])\n",
        "# display a 2D plot of the digit classes in the latent space\n",
        "x_test_encoded = encoder.predict(X_test, batch_size=batch_size)\n",
        "print(x_test_encoded.shape)\n",
        "print(x_test_encoded)\n",
        "print(y_test.shape)\n",
        "plt.figure(figsize=(8, 6), dpi=100)\n",
        "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=x_test_encoded[:, 0])\n",
        "plt.title('Variational Autoencoder')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_encoded = encoder.predict(df_norm, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "YEuHj8qFGCDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "tags=df['tag'].unique()\n",
        "le = LabelEncoder()\n",
        "le.fit(tags)\n",
        "encoded_tags=le.transform(tags)\n",
        "print(tags)\n",
        "print(encoded_tags)\n",
        "pca      = PCA(n_components=2)\n",
        "x_reduce = pca.fit_transform(x_test_encoded)\n",
        "print('Projecting %d-dimensional data to 2D' % x_test_encoded.shape[1])\n",
        "y=le.transform(df['tag'])\n",
        "plt.figure(figsize=(12,10))\n",
        "plt.scatter(x_reduce[:, 0], x_reduce[:, 1], c=y,\n",
        "            edgecolor='none', alpha=0.7, s=40,\n",
        "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "plt.colorbar()\n",
        "plt.title('NER. PCA projection')"
      ],
      "metadata": {
        "id": "0sUb3jpUEGoe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}